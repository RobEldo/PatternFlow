{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VQVAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpPpGMP09QsWzLQETSmcD3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcufqtAnjpps"
      },
      "source": [
        "# Install libraries\n",
        "%%capture\n",
        "!pip install tensorflow-probability\n",
        "!pip install imageio"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6G2euBbuoyk",
        "outputId": "d0039e10-a499-42e6-8e73-c29956727385"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "288u3VGmu-mk",
        "outputId": "41ea0233-536b-4764-8c7a-415190628f10"
      },
      "source": [
        "%cd gdrive/My Drive/COMP3710/PatternFlow/recognition/VQVAE_oasis3"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/My Drive/COMP3710/PatternFlow/recognition/VQVAE_oasis3'\n",
            "/content/gdrive/My Drive/COMP3710/PatternFlow/recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiPk8CT5vNkk",
        "outputId": "b2029244-7837-41ac-c052-d6ae6cc30bf0"
      },
      "source": [
        "! git config --global user.email \"rweldridge99@gmail.com\"\n",
        "! git commit -m \"Committed a starting file with progress on loading the OASIS dataset into tf.data.Dataset objects.\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master f5ae4ac] Committed a starting file with progress on loading the OASIS dataset into tf.data.Dataset objects.\n",
            " 1 file changed, 1 insertion(+)\n",
            " create mode 100644 recognition/VQVAE_oasis3/VQVAE.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GndRy8rT0ibY",
        "outputId": "5fae76ff-b45a-4af2-97de-4fbfa3c0ff59"
      },
      "source": [
        "! git push"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvAdSqb2jPNA",
        "outputId": "1ba41f78-399f-4a00-e6d5-e2db1690d429"
      },
      "source": [
        "# Import General Utilities\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "#Import tensorflow and its requirements\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Import plotting tools and image converters\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import PIL\n",
        "import imageio\n",
        "from IPython import display\n",
        "\n",
        "# Import file path variable handling\n",
        "from pathlib import Path\n",
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Constants\n",
        "# TO ADD %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bBDPN_5lBNQ",
        "outputId": "a942dbf8-18e1-46fa-fc1d-25e166527a06"
      },
      "source": [
        "# Download OASIS Dataset from the provided link\n",
        "\n",
        "def download_and_unzip(url, extract_to='.'): # Credit to Antoine Hebert\n",
        "    http_response = urlopen(url)\n",
        "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
        "    zipfile.extractall(path=extract_to)\n",
        "    http_response.close()\n",
        "\n",
        "root_dir = '/root/.keras/datasets'\n",
        "_URL = \"https://cloudstor.aarnet.edu.au/plus/s/tByzSZzvvVh0hZA/download\"\n",
        "download_and_unzip(_URL, extract_to=root_dir)\n",
        "#data_dir = tf.keras.utils.get_file('oasis3', origin=_URL, cache_subdir=root_dir, extract=True)\n",
        "data_dir = Path(root_dir) / 'keras_png_slices_data'\n",
        "\n",
        "print(f\"Data Directory is {data_dir}\")\n",
        "print(f\"Contents: \\n\\r {os.listdir(data_dir)}\")\n",
        "\n",
        "# Folder Directory Paths\n",
        "train_dir = data_dir / 'keras_png_slices_train'\n",
        "train_ans_dir = data_dir / 'keras_png_slices_seg_train'\n",
        "test_dir = data_dir / 'keras_png_slices_test'\n",
        "test_ans_dir = data_dir / 'keras_png_slices_seg_test'\n",
        "val_dir = data_dir / 'keras_png_slices_validate'\n",
        "val_ans_dir = data_dir / 'keras_png_slices_seg_validate'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Directory is /root/.keras/datasets/keras_png_slices_data\n",
            "Contents: \n",
            "\r ['keras_png_slices_train', 'keras_png_slices_seg_train', 'keras_png_slices_seg_test', 'keras_png_slices_validate', 'keras_png_slices_seg_validate', 'keras_png_slices_test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2eBbuK-sBc3",
        "outputId": "64796df0-23b8-4255-874a-a9afe444cc84"
      },
      "source": [
        "def count_files(dat_path):\n",
        "    return len(list(dat_path.glob('*.*')))\n",
        "\n",
        "train_ds_list = tf.data.Dataset.list_files(str(train_dir/'*.*'), shuffle=False)\n",
        "train_ds_list = train_ds_list.shuffle(count_files(train_dir), reshuffle_each_iteration=False)\n",
        "\n",
        "for f in train_ds_list.take(5):\n",
        "  print(f.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'/root/.keras/datasets/keras_png_slices_data/keras_png_slices_train/case_353_slice_19.nii.png'\n",
            "b'/root/.keras/datasets/keras_png_slices_data/keras_png_slices_train/case_318_slice_30.nii.png'\n",
            "b'/root/.keras/datasets/keras_png_slices_data/keras_png_slices_train/case_338_slice_9.nii.png'\n",
            "b'/root/.keras/datasets/keras_png_slices_data/keras_png_slices_train/case_256_slice_6.nii.png'\n",
            "b'/root/.keras/datasets/keras_png_slices_data/keras_png_slices_train/case_289_slice_3.nii.png'\n"
          ]
        }
      ]
    }
  ]
}